Homework 2
================
Malika Top
2024-10-02

## Problem 1

``` r
nyc_transit = 
  read_csv("data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") |> 
  janitor::clean_names()

nyc_subset =
  nyc_transit |> 
  select(line:entry, vending, ada) |> 
  mutate(
    entry = case_match(entry, "YES" ~ TRUE, "NO" ~ FALSE)
  )
# OR:
# nyc_subset =
#   nyc_transit |> 
#   select(line:entry, vending, ada) |> 
#   mutate(
#     entry = ifelse(entry == "YES", TRUE, FALSE )
#   )
```

This dataframe contains information about the NYC Transit system’s
entrances and for all stations and lines. Each row represents a station
and has information about which line it belongs to, the daytime routes
that serve that station, whether it has stairs at the entrance, if it is
ADA compliant, and presumably if it has an OMNY vending machine there.
If there is a repeat of a station, that means there are multiple
entrances/exits. For example, looking at the first few rows:

``` r
slice(nyc_subset, 1:3)
```

    ## # A tibble: 3 × 19
    ##   line     station_name station_latitude station_longitude route1 route2 route3
    ##   <chr>    <chr>                   <dbl>             <dbl> <chr>  <chr>  <chr> 
    ## 1 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 2 4 Avenue 25th St                  40.7             -74.0 R      <NA>   <NA>  
    ## 3 4 Avenue 36th St                  40.7             -74.0 N      R      <NA>  
    ## # ℹ 12 more variables: route4 <chr>, route5 <chr>, route6 <chr>, route7 <chr>,
    ## #   route8 <dbl>, route9 <dbl>, route10 <dbl>, route11 <dbl>,
    ## #   entrance_type <chr>, entry <lgl>, vending <chr>, ada <lgl>

Stations are identified by **station_name and line**, so the first row
is 25th St 4 Av. We see R in the `route1` column, which means that the R
train serves this station during the day. Looking at the third row
though, we see N in `route1` and R in `route2` which means both the N
and R train serve this station (but run different routes).

After reading the data in and accounting for NAs, I cleaned using the
`janitor` package’s `clean_names()` function, and select 19 of the
original 32 columns of interest. I then converted the `entry` variable
to be of `logical` type, rather than just character. The dimensions of
the dataset are 19 columns and 1868 rows. However, this data is **not
tidy** because the `route1`… `route11` values are spread across separate
columns, when it could be just one variable, say `route_number`.

``` r
nyc_dist_subset = 
  nyc_subset |> 
  mutate(
    station_id = paste(station_name, line) # stations are identified both by name and by line 
  ) |> 
  relocate(station_id, .after = station_name) |> 
  distinct(station_id, .keep_all = TRUE)

# lapply(nyc_dist_subset, class) # we see that routes 8, 9, 10, 11 are `numeric` class
tidy_stations =
  nyc_dist_subset |> 
  mutate(
    route8 = as.character(route8),
    route9 = as.character(route9),
    route10 = as.character(route10),
    route11 = as.character(route11)
  ) |> 
  pivot_longer(
    route1:route11,
    names_to = "route_number",
    values_to = "route_name"
  ) |> 
  relocate(route_number, route_name, .after = station_id) |> 
  drop_na(route_name)

head(tidy_stations)
```

    ## # A tibble: 6 × 11
    ##   line     station_name station_id      route_number route_name station_latitude
    ##   <chr>    <chr>        <chr>           <chr>        <chr>                 <dbl>
    ## 1 4 Avenue 25th St      25th St 4 Aven… route1       R                      40.7
    ## 2 4 Avenue 36th St      36th St 4 Aven… route1       N                      40.7
    ## 3 4 Avenue 36th St      36th St 4 Aven… route2       R                      40.7
    ## 4 4 Avenue 45th St      45th St 4 Aven… route1       R                      40.6
    ## 5 4 Avenue 53rd St      53rd St 4 Aven… route1       R                      40.6
    ## 6 4 Avenue 59th St      59th St 4 Aven… route1       N                      40.6
    ## # ℹ 5 more variables: station_longitude <dbl>, entrance_type <chr>,
    ## #   entry <lgl>, vending <chr>, ada <lgl>

### What proportion of station entrances / exits without vending allow entrance?

``` r
# Does entry == FALSE mean that it's an exit?
no_vending_entry = 
  nyc_subset |> 
  filter(vending == "NO") |> # 183 obs
  filter(entry == TRUE) # 69 obs
```

The proportion of station entrances/exits without vending that allow
entrance are $\frac{69}{183}$ = 0.377

### How many stations are ADA compliant?

``` r
ada_compliant = 
  tidy_stations |> 
  filter(ada==TRUE)
```

There are 293 stations compliant with ADA.

### How many distinct stations serve the A train? Of the stations that serve the A train, how many are ADA compliant?

``` r
service_A = filter(tidy_stations, route_name == "A")
ada_A = filter(service_A, ada == TRUE)
```

There are 60 distinct stations that serve the A. Of the stations serving
the A, there are 17 stations that are ADA compliant.

## Problem 2

### Reading and Cleaning

For importing the Trash Wheel data, the original .xlsx file has multiple
sheets, so we need to specify the corresponding trash wheel in the
`read_excel` function. I used `clean_names` to standardize the column
names, and got rid of the last row which was just the totals of all the
columns for all the dataframes. I rounded the `sports_balls` variable
since some of them were doubles, and converted it to be integers. For
Mr. Trash, I also had to convert the `year` column to an integer since
it was originally of character type.

To all of the individual tables, I added a column characterizing the
type of trash wheel the observation belonged to, and relocated that
variable to the beginning for better organization.

``` r
mr_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      range = "A2:N655", 
                      sheet=1) |> 
            janitor::clean_names() |> 
            filter(row_number() <= n() - 2) |>  #gets rid of the last row which is totals
            mutate(
              sports_balls = as.integer(round(sports_balls)),
              year = as.integer(year),
              trash_wheel = "mr"
            ) |> 
            relocate(trash_wheel)
prof_trash = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                      range = "A2:M123",
                      sheet=2) |> 
              janitor::clean_names()|> 
              filter(row_number() <= n() - 3) |> #gets rid of the last rows which
                                                #are totals or empty
              mutate(
                trash_wheel = "professor"
                )|> 
            relocate(trash_wheel)

gwynda = read_excel("data/202409 Trash Wheel Collection Data.xlsx", 
                    range = "A2:L266",
                    sheet=4) |> 
              janitor::clean_names()|> 
              filter(row_number() <= n()-1) |>  #gets rid of the last row which is totals
              mutate(
                trash_wheel = "gwynda"
              )|> 
            relocate(trash_wheel)
```

### Combining dataframes

``` r
combined_trash = 
  bind_rows(mr_trash, prof_trash, gwynda) 
combined_trash
```

    ## # A tibble: 1,032 × 15
    ##    trash_wheel dumpster month  year date                weight_tons
    ##    <chr>          <dbl> <chr> <dbl> <dttm>                    <dbl>
    ##  1 mr                 1 May    2014 2014-05-16 00:00:00        4.31
    ##  2 mr                 2 May    2014 2014-05-16 00:00:00        2.74
    ##  3 mr                 3 May    2014 2014-05-16 00:00:00        3.45
    ##  4 mr                 4 May    2014 2014-05-17 00:00:00        3.1 
    ##  5 mr                 5 May    2014 2014-05-17 00:00:00        4.06
    ##  6 mr                 6 May    2014 2014-05-20 00:00:00        2.71
    ##  7 mr                 7 May    2014 2014-05-21 00:00:00        1.91
    ##  8 mr                 8 May    2014 2014-05-28 00:00:00        3.7 
    ##  9 mr                 9 June   2014 2014-06-05 00:00:00        2.52
    ## 10 mr                10 June   2014 2014-06-11 00:00:00        3.76
    ## # ℹ 1,022 more rows
    ## # ℹ 9 more variables: volume_cubic_yards <dbl>, plastic_bottles <dbl>,
    ## #   polystyrene <dbl>, cigarette_butts <dbl>, glass_bottles <dbl>,
    ## #   plastic_bags <dbl>, wrappers <dbl>, sports_balls <int>, homes_powered <dbl>

``` r
total_weight = sum(pull(combined_trash, weight_tons))
total_homes = sum(pull(combined_trash, homes_powered), na.rm = TRUE)
```

After cleaning, I combined the cleaned individual trash wheel dataframes
into one final table. The total number of observations in the resulting
dataset is 1032. Key variables include:

- `weight_tons`: Combined, they collected a total of 3135.47 tons of
  trash.
- `homes_powered`: Combined, they powered 4.4821667^{4} homes.

#### Professor Trash Wheel and Gwynda

``` r
prof_trash_total_wt = sum(
  combined_trash |> 
  filter(trash_wheel == "professor") |> 
  select(weight_tons)
  )
gwynda_cigs_06_22 = 
  sum(
    combined_trash |> 
    filter(trash_wheel == "gwynda" &
           month == "June" 
         & year == 2022) |> 
    select(cigarette_butts)
  )
```

Professor Trash Wheel collected in total **246.74** tons of trash.
Gwynda collected **18120** cigarette butts in June of 2022.

## Problem 3

``` r
bakers_df = read_csv("data/gbb_datasets/bakers.csv",
                     na = c("NA", "UNKNOWN", "N/A", "")) |> 
  janitor::clean_names() |> 
  separate_wider_delim(baker_name, " ", names=c("first_name",
                                                "last_name")) 
bakers_df$first_name[bakers_df$first_name=="Jo"] <- "Joanne"  

bakes_df = read_csv("data/gbb_datasets/bakes.csv",
                    na = c("NA", "UNKNOWN", "N/A", ""))|> 
  janitor::clean_names() |> 
  rename(first_name = baker) |> 
  mutate(first_name = gsub('"', '', first_name))
bakes_df$first_name[bakes_df$first_name=="Jo"] <- "Joanne"
           
results_df = read_csv("data/gbb_datasets/results.csv", 
                      na = c("NA", "UNKNOWN", "N/A", ""),
                      skip=2) |> 
  janitor::clean_names() |> 
  rename(first_name = baker)
```

The `bakers_df` has information about the contestants, for which there
are 12 each series, and a total of 10 series. The number of episodes per
series ranges from 6-10. `bakes_df` contains information about how
someone did in a technical, the Signature they made, and the
Showstopper. Upon initial data exploration, we see that `results_df`
includes contestants up to Series 10, but `bakes_df` only has
information up to Series 8, which means we might want to apply any
mutative join starting with `results_df` so we don’t lose any additional
information. There are many missing NA values but that makes sense,
because once a contestant is eliminated, there should not be information
on their bakes/how they did.

For cleaning, I separated the original `baker_name` column in
`bakers_df` to have just first name, to match the other tables.

In thinking about how to combine these dataframes together, I considered
using `left_join` because I didn’t want to miss any data from
`results_df`, whereas using `right_join` or `full_join` would not be
necessary since we know that the `bakes_df` does not have all the
information to match `results_df` anyways.

``` r
comb_df = results_df |> 
  left_join(bakes_df, by = c("series", "episode", "first_name")) |> 
  left_join(bakers_df, by = c("series", "first_name"))
```

### Checking for completeness and correctness

``` r
anti_join(results_df, comb_df, by = c("series", "episode", "first_name"))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, first_name <chr>,
    ## #   technical <dbl>, result <chr>

``` r
anti_join(bakes_df, comb_df, by = c("series", "episode", "first_name"))
```

    ## # A tibble: 0 × 5
    ## # ℹ 5 variables: series <dbl>, episode <dbl>, first_name <chr>,
    ## #   signature_bake <chr>, show_stopper <chr>

``` r
anti_join(bakers_df, comb_df, by = c("series", "first_name"))
```

    ## # A tibble: 0 × 6
    ## # ℹ 6 variables: first_name <chr>, last_name <chr>, series <dbl>,
    ## #   baker_age <dbl>, baker_occupation <chr>, hometown <chr>

Checking the merged dataframe against `results_df`, we get an empty
table, so that one checks out. However, antijoining with `bakes_df`
returned an 8x5 tibble showing that “Jo” of Series 2 was left out. After
some sifting, I found out the reason why is because the same contestant
is named “Jo” in two dataframes but “Joanne” in another, so I had to go
back to clean that. After cleaning though, `anti_join` returns an empty
tibble, so our dataframes are now complete.

``` r
comb_df = 
  comb_df |> 
  relocate(signature_bake, .before = "technical") |> 
  relocate(result, .after = "show_stopper") |> 
  relocate(last_name, .after = "first_name")
comb_df
```

    ## # A tibble: 1,136 × 11
    ##    series episode first_name last_name signature_bake     technical show_stopper
    ##     <dbl>   <dbl> <chr>      <chr>     <chr>                  <dbl> <chr>       
    ##  1      1       1 Annetha    Mills     "Light Jamaican B…         2 Red, White …
    ##  2      1       1 David      Chambers  "Chocolate Orange…         3 Black Fores…
    ##  3      1       1 Edd        Kimber    "Caramel Cinnamon…         1 <NA>        
    ##  4      1       1 Jasminder  Randhawa  "Fresh Mango and …        NA <NA>        
    ##  5      1       1 Jonathan   Shepherd  "Carrot Cake with…         9 Three Tiere…
    ##  6      1       1 Louise     Brimelow  "Carrot and Orang…        NA Never Fail …
    ##  7      1       1 Miranda    Browne    "Triple Layered B…         8 Three Tiere…
    ##  8      1       1 Ruth       Clemens   "Three Tiered Lem…        NA Classic Cho…
    ##  9      1       1 Lea        Harris    "Cranberry and Pi…        10 Raspberries…
    ## 10      1       1 Mark       Whithers  "Sticky Marmalade…        NA Heart-shape…
    ## # ℹ 1,126 more rows
    ## # ℹ 4 more variables: result <chr>, baker_age <dbl>, baker_occupation <chr>,
    ## #   hometown <chr>

I relocated the columns and organized them so that they were in more
meaningful orders (like doing the different sections in order since the
signature is completed first, then the technical, then the showstopper).
I put the contestant hometown and occupation information at the end
though since they felt less relevant (perhaps less of a precursor and
more of a footnote).

For the final dataframe, each row is the result for each contestant in a
specific episode. It shows what that baker baked for their signature,
what they placed in the technical, what they baked for the showstopper,
and whether they were eliminated or named Star Baker. For final
episodes, the dataframe also tells us if that person won or was a
runner-up. There is also biographical information for each contestant.

There are many NAs in the dataframe though because for example if a
contestant was eliminated in episode 3, they would still exist as a row
for episode 4, 5, 6, etc., but since they didn’t make anything, they
would have NAs in the columns for showstopper/technical, etc.

``` r
write.csv(comb_df, 
          file = "data/gbb_datasets/combined_gbb.csv")
```

``` r
series_5_10 = 
  comb_df |> 
  filter(series == 5 | series == 6 | series == 7 |
           series == 8 | series == 9 | series == 10) |> 
  filter(result == "STAR BAKER" | result == "WINNER") |> 
  knitr::kable() |> 
  kable_paper() |> 
  scroll_box(width = "1000px", height = "500px")
  
series_5_10
```

<div style="border: 1px solid #ddd; padding: 0px; overflow-y: scroll; height:500px; overflow-x: scroll; width:1000px; ">
<table class=" lightable-paper" style='font-family: "Arial Narrow", arial, helvetica, sans-serif; margin-left: auto; margin-right: auto;'>
<thead>
<tr>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
series
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
episode
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
first_name
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
last_name
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
signature_bake
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
technical
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
show_stopper
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
result
</th>
<th style="text-align:right;position: sticky; top:0; background-color: #FFFFFF;">
baker_age
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
baker_occupation
</th>
<th style="text-align:left;position: sticky; top:0; background-color: #FFFFFF;">
hometown
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Nancy
</td>
<td style="text-align:left;">
Birtwhistle
</td>
<td style="text-align:left;">
Coffee and Hazelnut Swiss Roll
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Jaffa Orange Cakes
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
60
</td>
<td style="text-align:left;">
Retired Practice Manager
</td>
<td style="text-align:left;">
Barton-upon-Humber, Lincolnshire
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:left;">
Rosemary Seeded Crackers
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Pirates!
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Luis
</td>
<td style="text-align:left;">
Troyano
</td>
<td style="text-align:left;">
Opposites Attract Rolls
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Roscón de Reyes
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
42
</td>
<td style="text-align:left;">
Graphic Designer
</td>
<td style="text-align:left;">
Poynton, Cheshire
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:left;">
Black Forest Chocolate Fondants
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Tiramisu Baked Alaska
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
38
</td>
<td style="text-align:left;">
Builder
</td>
<td style="text-align:left;">
Mill Hill, London
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
5
</td>
<td style="text-align:left;">
Kate
</td>
<td style="text-align:left;">
Henry
</td>
<td style="text-align:left;">
Rhubarb and Custard Tart
</td>
<td style="text-align:right;">
3
</td>
<td style="text-align:left;">
Rhubarb, Prune and Apple Pies
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
41
</td>
<td style="text-align:left;">
Furniture Restorer
</td>
<td style="text-align:left;">
Brighton, East Sussex
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
Chetna
</td>
<td style="text-align:left;">
Makan
</td>
<td style="text-align:left;">
Orange Savarin with Cinnamon Cream
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
Almond Liqueur Dobos Torte with Chocolate Caramel Buttercream
</td>
<td style="text-align:left;">
STAR BAKER
</td>
<td style="text-align:right;">
35
</td>
<td style="text-align:left;">
Fashion Designer
</td>
<td style="text-align:left;">
Broadstairs, Kent
</td>
</tr>
<tr>
<td style="text-align:right;">
5
</td>
<td style="text-align:right;">
7
</td>
<td style="text-align:left;">
Richard
</td>
<td style="text-align:left;">
Burr
</td>
<td style="text-align:left;">
Minted Lamb Pasties
</td>
<td style="text-align:right;">
1
</td>
<td style="text-align:left;">
Stair of Éclairs (Lavender and Blueberry & Rose and Raspberry Éclairs)
</td>
<td style="text-align:left;">
\|STAR BAKER
</td>
<td style="text-align:right;">

       3

</tbody>
</table>
</div>

Looking at the table, some patterns we can see are that most of the star
bakers that week did good on the technical and people who won the series
had gotten star baker at least once (at lease for series 5-8).

``` r
tech_scores = 
    comb_df |> 
    filter(series == 5 | series == 6 | series == 7 |
           series == 8 | series == 9 | series == 10) |>
    filter(result == "STAR BAKER") |> 
    pull(technical)
```

The average technical score for people who had been star baker that week
was 2.888. **\[NOTE: I had to calculate this the long way because there
was an error when I tried to use `filter` on the `series_5_10` table
since I had already applied `kable()` to it\]**

### Viewers

Import, clean, tidy, and organize the viewership data in viewers.csv.
Show the first 10 rows of this dataset. What was the average viewership
in Season 1? In Season 5?

``` r
viewers_df = read_csv("data/gbb_datasets/viewers.csv") |> 
  janitor::clean_names() |> 
  pivot_longer(
    cols = series_1:series_10,
    names_to = "series",
    values_to = "viewers_in_millions",
    names_prefix = "series_"
  ) |> 
  mutate(series = as.numeric(series)) |> 
  relocate(series) |> 
  arrange(series)
```

    ## Rows: 10 Columns: 11
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (11): Episode, Series 1, Series 2, Series 3, Series 4, Series 5, Series ...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
viewers_df[1:10, ]
```

    ## # A tibble: 10 × 3
    ##    series episode viewers_in_millions
    ##     <dbl>   <dbl>               <dbl>
    ##  1      1       1                2.24
    ##  2      1       2                3   
    ##  3      1       3                3   
    ##  4      1       4                2.6 
    ##  5      1       5                3.03
    ##  6      1       6                2.75
    ##  7      1       7               NA   
    ##  8      1       8               NA   
    ##  9      1       9               NA   
    ## 10      1      10               NA

``` r
s1_views = 
  viewers_df |> 
  filter(series == 1) |> 
  pull(viewers_in_millions)

s5_views = 
  viewers_df |> 
  filter(series == 5) |> 
  pull(viewers_in_millions)
```

The average viewership for series 1 is 2.77 million and 10.0393 million
for series 5, so we see a great growth in viewers, assuming that the NA
values for series 1 are similar to the rest.
